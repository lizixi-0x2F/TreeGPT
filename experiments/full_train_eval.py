#!/usr/bin/env python3

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
import logging
import json
import time
from pathlib import Path
from tqdm import tqdm
import numpy as np
from typing import Dict, List, Union, Tuple

from arc_treegpt import ARCTreeGPT, ARCGridTokenizer, ARCDataset, collate_fn, compute_streaming_autoregressive_loss, compute_streaming_autoregressive_accuracy

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def calculate_token_accuracy(logits, targets, ignore_index=16):
    """è®¡ç®—tokençº§åˆ«å‡†ç¡®ç‡"""
    predictions = torch.argmax(logits, dim=-1)
    mask = (targets != ignore_index)
    correct = (predictions == targets) & mask
    return correct.sum().item() / mask.sum().item() if mask.sum() > 0 else 0.0


def calculate_full_accuracy(logits, targets, ignore_index=16):
    """è®¡ç®—å®Œæ•´åºåˆ—å‡†ç¡®ç‡ - åªæœ‰å½“æ•´ä¸ªåºåˆ—éƒ½æ­£ç¡®æ—¶æ‰ç®—å¯¹"""
    predictions = torch.argmax(logits, dim=-1)  # [batch_size, seq_len]
    mask = (targets != ignore_index)  # [batch_size, seq_len]
    
    # å¯¹æ¯ä¸ªæ ·æœ¬æ£€æŸ¥æ˜¯å¦å®Œå…¨æ­£ç¡®
    batch_size = targets.size(0)
    full_correct = 0
    
    for i in range(batch_size):
        sample_mask = mask[i]
        if sample_mask.sum() == 0:  # è·³è¿‡æ²¡æœ‰æœ‰æ•ˆtokençš„æ ·æœ¬
            continue
        sample_predictions = predictions[i][sample_mask]
        sample_targets = targets[i][sample_mask]
        # åªæœ‰å½“æ‰€æœ‰tokenéƒ½æ­£ç¡®æ—¶æ‰ç®—å¯¹
        if torch.all(sample_predictions == sample_targets):
            full_correct += 1
    
    return full_correct / batch_size if batch_size > 0 else 0.0




def train_full_model():
    """å®Œæ•´è®­ç»ƒARC TreeGPTæ¨¡å‹ - ä½¿ç”¨è‡ªå›å½’è®­ç»ƒ"""
    # å¿…é¡»ä½¿ç”¨MPSè®¾å¤‡
    if not torch.backends.mps.is_available():
        raise RuntimeError("MPS device not available!")
    
    device = torch.device('mps')
    logger.info(f"Using device: {device}")
    
    # æ¨¡å‹é…ç½® - é€‚é…MPSå†…å­˜é™åˆ¶
    model_config = {
        'vocab_size': 17,
        'd_model': 128,  # å‡å°ä»¥é€‚åº”MPSå†…å­˜
        'n_heads': 2,
        'n_layers': 2,
        'max_seq_len': 8192,  # æŒ‰ç”¨æˆ·è¦æ±‚è®¾ç½®ä¸º2048
        'dropout': 0.2,
        'tree_iterations': 2,
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = ARCTreeGPT(**model_config).to(device)
    
    # ä¸ºTå‚æ•°è®¾ç½®æ›´é«˜çš„å­¦ä¹ ç‡
    tree_params = []
    other_params = []
    
    for name, param in model.named_parameters():
        if 'tree_ffn.T' in name:
            tree_params.append(param)
        else:
            other_params.append(param)
    
    optimizer = torch.optim.AdamW([
        {'params': other_params, 'lr': 1e-4},
        {'params': tree_params, 'lr': 1e-3}  # Tå‚æ•°ä½¿ç”¨10å€å­¦ä¹ ç‡
    ], weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
    
    logger.info(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # åŠ è½½è®­ç»ƒæ•°æ® - ä½¿ç”¨å¸¦è§£å†³æ–¹æ¡ˆçš„æ•°æ®é›†
    train_dataset = ARCDataset(
        '../arc-prize-2025/arc-agi_training_challenges.json', 
        '../arc-prize-2025/arc-agi_training_solutions.json',  # æ·»åŠ è§£å†³æ–¹æ¡ˆæ–‡ä»¶
        max_length=8192  # å¢åŠ åˆ°8192ä»¥é¿å…æˆªæ–­è¿‡å¤šæ ·æœ¬
    )
    train_loader = DataLoader(
        train_dataset, 
        batch_size=1,  # æŒ‰ç”¨æˆ·è¦æ±‚è®¾ç½®ä¸º1
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=0
    )
    
    # åŠ è½½è¯„ä¼°æ•°æ®ç”¨äºéªŒè¯
    eval_dataset = ARCDataset(
        '../arc-prize-2025/arc-agi_evaluation_challenges.json', 
        '../arc-prize-2025/arc-agi_evaluation_solutions.json',  # æ·»åŠ è§£å†³æ–¹æ¡ˆæ–‡ä»¶
        max_length=8192
    )
    
    tokenizer = ARCGridTokenizer()
    
    logger.info(f"Training samples: {len(train_dataset)}")
    logger.info(f"Evaluation samples: {len(eval_dataset)}")
    
    # è®­ç»ƒå¾ªç¯
    model.train()
    epochs = 1
    best_val_acc = 0.0
    global_step = 0  # å…¨å±€æ­¥æ•°è®¡æ•°å™¨
    validation_interval = 500  # æ¯500æ­¥éªŒè¯ä¸€æ¬¡
    
    training_history = {
        'epochs': [],
        'train_losses': [],
        'token_accs': [],
        'full_accs': [],
        'val_token_accs': [],
        'val_full_accs': [],
        'lr': [],
        'steps': []  # è®°å½•éªŒè¯æ—¶çš„æ­¥æ•°
    }
    
    for epoch in range(epochs):
        epoch_loss = 0
        epoch_token_acc = 0
        epoch_full_acc = 0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for batch in progress_bar:
            global_step += 1  # å¢åŠ å…¨å±€æ­¥æ•°
            
            # ç§»åŠ¨æ‰¹æ¬¡æ•°æ®åˆ°è®¾å¤‡
            for key in batch:
                if isinstance(batch[key], torch.Tensor):
                    batch[key] = batch[key].to(device)
            
            optimizer.zero_grad()
            
            # ä½¿ç”¨é«˜æ•ˆçš„æµå¼è‡ªå›å½’æŸå¤±è®¡ç®—
            loss = compute_streaming_autoregressive_loss(
                model, batch, tokenizer, device
            )
            
            if loss.item() > 0:  # åªæœ‰å½“æœ‰æœ‰æ•ˆæ ·æœ¬æ—¶æ‰æ›´æ–°
                # è®¡ç®—çœŸå®çš„å‡†ç¡®ç‡æŒ‡æ ‡
                token_acc, full_acc, valid_samples = compute_streaming_autoregressive_accuracy(
                    model, batch, tokenizer, device
                )
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                
                epoch_loss += loss.item()
                epoch_token_acc += token_acc
                epoch_full_acc += full_acc
                num_batches += 1
                
                progress_bar.set_postfix({
                    'step': global_step,
                    'loss': f'{loss.item():.4f}',
                    'token_acc': f'{token_acc:.4f}',
                    'full_acc': f'{full_acc:.4f}',
                    'lr': f'{optimizer.param_groups[0]["lr"]:.6f}',
                    'T_avg': f'{torch.mean(torch.stack([block.tree_ffn.T for block in model.blocks])).item():.2f}'
                })
            else:
                # è·³è¿‡æ— æ•ˆæ ·æœ¬
                progress_bar.set_postfix({
                    'step': global_step,
                    'status': 'skipped (no solution)',
                    'lr': f'{optimizer.param_groups[0]["lr"]:.6f}'
                })
            
            # æ¯500æ­¥è¿›è¡ŒéªŒè¯
            if global_step % validation_interval == 0:
                logger.info(f"\nğŸ¯ Step {global_step}: Running validation...")
                val_token_acc, val_full_acc, val_samples = evaluate_during_training(
                    model, eval_dataset, tokenizer, device, max_samples=120
                )
                
                # è®°å½•éªŒè¯å†å²
                training_history['steps'].append(global_step)
                training_history['epochs'].append(epoch + 1)
                training_history['train_losses'].append(loss.item())  # å½“å‰æ­¥çš„æŸå¤±
                training_history['token_accs'].append(token_acc)      # å½“å‰æ­¥çš„tokenå‡†ç¡®ç‡
                training_history['full_accs'].append(full_acc)        # å½“å‰æ­¥çš„å®Œæ•´å‡†ç¡®ç‡
                training_history['val_token_accs'].append(val_token_acc)
                training_history['val_full_accs'].append(val_full_acc)
                training_history['lr'].append(optimizer.param_groups[0]['lr'])
                
                logger.info(f"Step {global_step} (Epoch {epoch+1}):")
                logger.info(f"  Current Loss: {loss.item():.4f}")
                logger.info(f"  Current Token Acc: {token_acc:.4f}")
                logger.info(f"  Current Full Acc: {full_acc:.4f}")
                logger.info(f"  Val Token Acc: {val_token_acc:.4f} ({val_samples} samples)")
                logger.info(f"  Val Full Acc: {val_full_acc:.4f} ({val_samples} samples)")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯å®Œæ•´åºåˆ—å‡†ç¡®ç‡ï¼‰
                if val_full_acc > best_val_acc:
                    best_val_acc = val_full_acc
                    torch.save({
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'epoch': epoch,
                        'step': global_step,
                        'train_loss': loss.item(),
                        'token_acc': token_acc,
                        'full_acc': full_acc,
                        'val_token_acc': val_token_acc,
                        'val_full_acc': val_full_acc,
                        'config': model_config
                    }, 'best_arc_treegpt.pth')
                    logger.info(f"ğŸ’¾ Saved best model at step {global_step} with val full acc {val_full_acc:.4f}")
                
                logger.info("-" * 60)
        
        scheduler.step()
        
        # è®¡ç®—epochå¹³å‡æŒ‡æ ‡ï¼ˆä»…ç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼‰
        avg_loss = epoch_loss / num_batches
        avg_token_acc = epoch_token_acc / num_batches
        avg_full_acc = epoch_full_acc / num_batches
        current_lr = optimizer.param_groups[0]['lr']
        
        # è®¡ç®—Tå€¼ç»Ÿè®¡
        T_values = [block.tree_ffn.T.item() for block in model.blocks]
        avg_T = sum(T_values) / len(T_values)
        min_T = min(T_values)
        max_T = max(T_values)
        
        logger.info(f"\nğŸ“Š Epoch {epoch+1}/{epochs} Summary:")
        logger.info(f"  Average Train Loss: {avg_loss:.4f}")
        logger.info(f"  Average Train Token Acc: {avg_token_acc:.4f}")
        logger.info(f"  Average Train Full Acc: {avg_full_acc:.4f}")
        logger.info(f"  LR: {current_lr:.6f}")
        logger.info(f"  Tree Iterations: avg={avg_T:.2f}, min={min_T:.2f}, max={max_T:.2f}")
        logger.info(f"  Total Steps: {global_step}")
        
        logger.info("=" * 60)
    
    # ä¿å­˜è®­ç»ƒå†å²
    with open('training_history.json', 'w') as f:
        json.dump(training_history, f, indent=2)
    
    logger.info("âœ… Training completed!")
    logger.info(f"Best validation accuracy: {best_val_acc:.4f}")
    return model


def compute_autoregressive_accuracy(model, batch, tokenizer, device, max_gen_length=50):
    """
    è®¡ç®—è‡ªå›å½’æ¨¡å¼ä¸‹çš„çœŸå®å‡†ç¡®ç‡
    """
    input_ids = batch['input_ids']
    input_lengths = batch['input_lengths']
    has_solutions = batch['has_solutions']
    
    batch_size = input_ids.size(0)
    total_tokens = 0
    correct_tokens = 0
    correct_sequences = 0
    valid_samples = 0
    
    for b in range(batch_size):
        if not has_solutions[b]:
            continue
            
        full_seq = input_ids[b]
        input_length = input_lengths[b]
        seq_length = (full_seq != 16).sum().item()
        
        if seq_length <= input_length:
            continue
            
        input_seq = full_seq[:input_length].unsqueeze(0)
        target_seq = full_seq[input_length:seq_length]
        
        # è‡ªå›å½’ç”Ÿæˆé¢„æµ‹
        current_seq = input_seq.clone()
        predictions = []
        
        for step in range(min(len(target_seq), max_gen_length)):
            if current_seq.size(1) > model.max_seq_len:
                current_seq = current_seq[:, -model.max_seq_len:]
            
            logits = model(current_seq)
            next_token_logits = logits[0, -1, :]
            predicted_token = torch.argmax(next_token_logits, dim=-1)
            predictions.append(predicted_token.item())
            
            # ä½¿ç”¨é¢„æµ‹çš„tokenç»§ç»­ç”Ÿæˆ
            current_seq = torch.cat([current_seq, predicted_token.unsqueeze(0).unsqueeze(0)], dim=1)
        
        # è®¡ç®—å‡†ç¡®ç‡
        target_tokens = target_seq[:len(predictions)].cpu().tolist()
        
        # Tokençº§å‡†ç¡®ç‡
        correct_in_seq = sum(1 for pred, target in zip(predictions, target_tokens) if pred == target)
        total_tokens += len(target_tokens)
        correct_tokens += correct_in_seq
        
        # åºåˆ—çº§å‡†ç¡®ç‡
        if predictions == target_tokens:
            correct_sequences += 1
        
        valid_samples += 1
    
    token_acc = correct_tokens / total_tokens if total_tokens > 0 else 0.0
    seq_acc = correct_sequences / valid_samples if valid_samples > 0 else 0.0
    
    return token_acc, seq_acc, valid_samples


def compute_autoregressive_accuracy(model, batch, tokenizer, device, max_gen_length=None):
    """
    é‡å®šä¹‰å‡½æ•°ä»¥ä½¿ç”¨æµå¼è‡ªå›å½’ç‰ˆæœ¬
    """
    return compute_streaming_autoregressive_accuracy(model, batch, tokenizer, device)


def evaluate_during_training(model, eval_dataset, tokenizer, device, max_samples=120):
    """è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¿«é€ŸéªŒè¯è¯„ä¼° - ä½¿ç”¨çœŸå®çš„è‡ªå›å½’å‡†ç¡®ç‡è®¡ç®—"""
    model.eval()
    total_loss = 0
    total_token_acc = 0
    total_full_acc = 0
    num_batches = 0
    
    # åˆ›å»ºä¸´æ—¶æ•°æ®åŠ è½½å™¨
    eval_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)
    
    with torch.no_grad():
        for i, batch in enumerate(eval_loader):
            if i >= max_samples:
                break
            
            try:
                # ç§»åŠ¨æ‰¹æ¬¡æ•°æ®åˆ°è®¾å¤‡
                for key in batch:
                    if isinstance(batch[key], torch.Tensor):
                        batch[key] = batch[key].to(device)
                
                # è®¡ç®—æŸå¤±
                loss = compute_streaming_autoregressive_loss(
                    model, batch, tokenizer, device
                )
                
                # è®¡ç®—çœŸå®å‡†ç¡®ç‡
                token_acc, seq_acc, valid_samples = compute_streaming_autoregressive_accuracy(
                    model, batch, tokenizer, device
                )
                
                if loss.item() > 0 and valid_samples > 0:
                    total_loss += loss.item()
                    total_token_acc += token_acc
                    total_full_acc += seq_acc
                    num_batches += 1
                    
            except Exception as e:
                continue
    
    model.train()
    
    avg_token_acc = total_token_acc / num_batches if num_batches > 0 else 0.0
    avg_full_acc = total_full_acc / num_batches if num_batches > 0 else 0.0
    
    return avg_token_acc, avg_full_acc, num_batches


def evaluate_model(model_path='best_arc_treegpt.pth'):
    """è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ - ä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„tokençº§è¯„ä¼°é€»è¾‘"""
    device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"Using device: {device}")
    
    # åŠ è½½æ¨¡å‹
    checkpoint = torch.load(model_path, map_location=device)
    model_config = checkpoint['config']
    
    model = ARCTreeGPT(**model_config).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    logger.info(f"Loaded model from {model_path}")
    logger.info(f"Best training metrics:")
    logger.info(f"  Train Loss: {checkpoint.get('train_loss', 'N/A'):.4f}")
    logger.info(f"  Token Acc: {checkpoint.get('token_acc', 'N/A'):.4f}")
    logger.info(f"  Full Acc: {checkpoint.get('full_acc', 'N/A'):.4f}")
    logger.info(f"  Val Token Acc: {checkpoint.get('val_token_acc', 'N/A'):.4f}")
    logger.info(f"  Val Full Acc: {checkpoint.get('val_full_acc', 'N/A'):.4f}")
    if 'step' in checkpoint:
        logger.info(f"  Best model saved at step: {checkpoint['step']}")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_dataset = ARCDataset('arc-prize-2025/arc-agi_test_challenges.json', max_length=8192)
    tokenizer = ARCGridTokenizer()
    
    logger.info(f"Loaded {len(test_dataset)} test samples")
    
    # ä½¿ç”¨ä¸è®­ç»ƒè¯„ä¼°ç›¸åŒçš„é€»è¾‘
    logger.info("ğŸ¯ Starting evaluation with training-consistent metrics...")
    
    # ç›´æ¥è°ƒç”¨è®­ç»ƒä¸­ä½¿ç”¨çš„è¯„ä¼°å‡½æ•°
    test_token_acc, test_full_acc, test_samples = evaluate_during_training(
        model, test_dataset, tokenizer, device, max_samples=len(test_dataset)
    )
    
    logger.info(f"\nğŸ“Š Final Evaluation Results (Training-Consistent):")
    logger.info(f"  Test Token Accuracy: {test_token_acc:.4f}")
    logger.info(f"  Test Full Sequence Accuracy: {test_full_acc:.4f}")
    logger.info(f"  Evaluated Samples: {test_samples}")
    
    # ä¿å­˜ç»“æœ
    results = {
        'model_path': model_path,
        'test_token_accuracy': test_token_acc,
        'test_full_sequence_accuracy': test_full_acc,
        'evaluated_samples': test_samples,
        'training_metrics': {
            'train_loss': checkpoint.get('train_loss', None),
            'token_acc': checkpoint.get('token_acc', None),
            'full_acc': checkpoint.get('full_acc', None),
            'val_token_acc': checkpoint.get('val_token_acc', None),
            'val_full_acc': checkpoint.get('val_full_acc', None),
            'step': checkpoint.get('step', None)
        }
    }
    
    results_file = 'evaluation_results.json'
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"ğŸ’¾ Results saved to {results_file}")
    
    return results


def decode_sequences_to_submission(
    sequences: Dict[str, List[np.ndarray]], 
    output_file: str = "submission.json"
) -> Dict:
    """
    Decode sequences into ARC Prize 2025 submission format.
    
    Args:
        sequences: Dictionary mapping task_id to list of predicted grids
                  Each grid should be a 2D numpy array or nested list
        output_file: Path to save the submission JSON file
    
    Returns:
        Dictionary in submission format
    """
    submission = {}
    
    for task_id, predictions in sequences.items():
        task_submissions = []
        
        # Handle multiple test cases for the same task
        for pred in predictions:
            # Convert numpy array to list if needed
            if isinstance(pred, np.ndarray):
                grid = pred.tolist()
            else:
                grid = pred
            
            # Create attempt structure
            test_case = {
                "attempt_1": grid,
                "attempt_2": grid  # Using same grid for both attempts
            }
            task_submissions.append(test_case)
        
        submission[task_id] = task_submissions
    
    # Save to file
    with open(output_file, 'w') as f:
        json.dump(submission, f, separators=(',', ':'))
    
    return submission


def decode_single_predictions_to_submission(
    predictions: Dict[str, Union[np.ndarray, List[List[int]]]], 
    output_file: str = "submission.json"
) -> Dict:
    """
    Decode single predictions per task into submission format.
    
    Args:
        predictions: Dictionary mapping task_id to single predicted grid
        output_file: Path to save the submission JSON file
    
    Returns:  
        Dictionary in submission format
    """
    submission = {}
    
    for task_id, pred in predictions.items():
        # Convert numpy array to list if needed
        if isinstance(pred, np.ndarray):
            grid = pred.tolist()
        else:
            grid = pred
            
        # Create single test case with both attempts using same prediction
        test_case = {
            "attempt_1": grid,
            "attempt_2": grid
        }
        
        submission[task_id] = [test_case]
    
    # Save to file
    with open(output_file, 'w') as f:
        json.dump(submission, f, separators=(',', ':'))
    
    return submission


def decode_dual_predictions_to_submission(
    predictions: Dict[str, Tuple[Union[np.ndarray, List], Union[np.ndarray, List]]], 
    output_file: str = "submission.json"
) -> Dict:
    """
    Decode dual predictions (attempt_1, attempt_2) per task into submission format.
    
    Args:
        predictions: Dictionary mapping task_id to tuple of (attempt_1, attempt_2) grids
        output_file: Path to save the submission JSON file
    
    Returns:
        Dictionary in submission format  
    """
    submission = {}
    
    for task_id, (attempt_1, attempt_2) in predictions.items():
        # Convert numpy arrays to lists if needed
        if isinstance(attempt_1, np.ndarray):
            grid_1 = attempt_1.tolist()
        else:
            grid_1 = attempt_1
            
        if isinstance(attempt_2, np.ndarray):
            grid_2 = attempt_2.tolist()
        else:
            grid_2 = attempt_2
            
        # Create test case with both attempts
        test_case = {
            "attempt_1": grid_1,
            "attempt_2": grid_2
        }
        
        submission[task_id] = [test_case]
    
    # Save to file
    with open(output_file, 'w') as f:
        json.dump(submission, f, separators=(',', ':'))
    
    return submission


def generate_predictions_for_submission(model_path='best_arc_treegpt.pth'):
    """
    Generate predictions from trained model and create submission file.
    """
    device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"ğŸ¯ Generating predictions for submission using device: {device}")
    
    # Load model
    checkpoint = torch.load(model_path, map_location=device)
    model_config = checkpoint['config']
    
    model = ARCTreeGPT(**model_config).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # Load TEST dataset for submission (not evaluation dataset)
    test_dataset = ARCDataset('arc-prize-2025/arc-agi_test_challenges.json', max_length=8192)
    tokenizer = ARCGridTokenizer()
    
    logger.info(f"Generating predictions for {len(test_dataset)} test tasks...")
    
    predictions = {}
    
    # Create data loader for test data
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)
    
    with torch.no_grad():
        for i, batch in enumerate(tqdm(test_loader, desc="Generating predictions")):
            try:
                input_ids = batch['input_ids'].to(device)
                seq_lengths = batch['seq_lengths']
                
                # Get the actual task ID from the batch
                task_id = batch['sample_ids'][0]  # Extract task ID from batch (note: plural 'sample_ids')
                
                # Generate prediction using the model
                # This is a simplified prediction - you might want to implement beam search or sampling
                logits = model(input_ids[:, :-1])
                prediction_tokens = torch.argmax(logits, dim=-1)
                
                # Convert tokens back to grid format
                # This is a placeholder - you'll need to implement proper token-to-grid conversion
                # based on your tokenizer's decode method
                try:
                    # Decode the prediction tokens to grid
                    pred_grid = tokenizer.decode_sequence(prediction_tokens[0].cpu().numpy())
                    
                    # Store prediction (using simple grid for now)
                    if isinstance(pred_grid, list) and len(pred_grid) > 0:
                        predictions[task_id] = pred_grid
                    else:
                        # Fallback to dummy prediction
                        predictions[task_id] = [[0, 0], [0, 0]]
                        
                except Exception as decode_error:
                    logger.warning(f"Failed to decode prediction for task {task_id}: {decode_error}")
                    # Use dummy prediction as fallback
                    predictions[task_id] = [[0, 0], [0, 0]]
                    
            except Exception as e:
                logger.warning(f"Failed to generate prediction for sample {i}: {e}")
                # Use dummy prediction with actual task ID if available
                try:
                    task_id = batch['sample_ids'][0] if 'sample_ids' in batch else f"task_{i:08d}"
                except:
                    task_id = f"task_{i:08d}"
                predictions[task_id] = [[0, 0], [0, 0]]
                continue
    
    logger.info(f"Generated {len(predictions)} predictions")
    
    # Create submission using the decoder
    submission = decode_single_predictions_to_submission(
        predictions,
        output_file="submission.json"
    )
    
    logger.info("âœ… Submission file created: submission.json")
    return submission


def create_example_submission():
    """
    Create an example submission file with the correct format for testing.
    """
    logger.info("ğŸ¯ Creating example submission file...")
    
    # Load sample submission to get task IDs
    try:
        with open('arc-prize-2025/sample_submission.json', 'r') as f:
            sample_submission = json.load(f)
        
        # Create predictions with same structure but different values
        example_predictions = {}
        for task_id in sample_submission.keys():
            # Generate a simple pattern as example prediction
            example_grid = [[1, 2, 1], [2, 1, 2], [1, 2, 1]]  # Simple checkerboard pattern
            example_predictions[task_id] = example_grid
        
        # Generate submission
        submission = decode_single_predictions_to_submission(
            example_predictions,
            output_file="example_submission.json"
        )
        
        logger.info("âœ… Example submission created: example_submission.json")
        logger.info(f"   Created predictions for {len(example_predictions)} tasks")
        
        return submission
        
    except Exception as e:
        logger.error(f"Failed to create example submission: {e}")
        return None


def full_pipeline():
    """è¿è¡Œå®Œæ•´çš„è®­ç»ƒ+è¯„ä¼°+æäº¤æµç¨‹"""
    logger.info("ğŸš€ Starting Full ARC TreeGPT Pipeline with Submission Generation")
    start_time = time.time()
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    if Path('best_arc_treegpt.pth').exists():
        logger.info("Found existing model, skipping training...")
        logger.info("Delete 'best_arc_treegpt.pth' to retrain from scratch")
    else:
        logger.info("Training new model...")
        train_full_model()
    
    # è¯„ä¼°æ¨¡å‹
    logger.info("\n" + "="*60)
    logger.info("Starting Final Evaluation")
    logger.info("="*60)
    
    evaluate_model()
    
    # ç”Ÿæˆæäº¤æ–‡ä»¶
    logger.info("\n" + "="*60)
    logger.info("Generating Submission Files")
    logger.info("="*60)
    
    # Generate predictions and create submission
    try:
        if Path('best_arc_treegpt.pth').exists():
            logger.info("ğŸ¯ Generating predictions from trained model...")
            generate_predictions_for_submission()
        else:
            logger.warning("No trained model found, creating example submission instead")
            
        # Always create an example submission for reference
        logger.info("ğŸ¯ Creating example submission for reference...")
        create_example_submission()
        
        logger.info("\nğŸ“ Generated Files:")
        if Path('arc_treegpt_submission.json').exists():
            logger.info("  âœ… arc_treegpt_submission.json - Model predictions")
        if Path('example_submission.json').exists():
            logger.info("  âœ… example_submission.json - Example format")
        
    except Exception as e:
        logger.error(f"Failed to generate submissions: {e}")
        logger.info("Creating fallback example submission...")
        create_example_submission()
    
    total_time = time.time() - start_time
    logger.info(f"\nğŸ‰ Pipeline completed in {total_time:.2f} seconds")
    
    # Final summary
    logger.info("\n" + "="*60)
    logger.info("PIPELINE SUMMARY")
    logger.info("="*60)
    logger.info("âœ… Model trained and evaluated")
    logger.info("âœ… Submission files generated")
    logger.info("ğŸ“Š Check 'evaluation_results.json' for detailed metrics")
    logger.info("ğŸ“ Check 'training_history.json' for training progress")
    logger.info("ğŸš€ Ready for ARC Prize 2025 submission!")
    logger.info("="*60)


if __name__ == "__main__":
    full_pipeline()